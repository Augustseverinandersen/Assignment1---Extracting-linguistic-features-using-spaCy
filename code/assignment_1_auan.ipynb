{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a spacy NLP class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\") # loads the entire model spacy into the variable nlp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for finding Rel Freq and Unique "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_attributes(directory): # Making a function called find_attributes with the parameter folderpath\n",
    "    \n",
    "    \n",
    "    # Making a for loop that finds each file and the path to that file, and saves it in a variable folder_path\n",
    "    # os.listdir makes a list of the specified directory with all the files in the directory.\n",
    "    # os.path.join joins the \"file\" to the path for the file.\n",
    "    for folder_name in os.listdir(directory): \n",
    "        folder_path = os.path.join(directory, folder_name)\n",
    "\n",
    "        # Start by checking if the new variable is a directory, if true it moves on and finds the path to each file in the subfolder.\n",
    "        if os.path.isdir(folder_path):\n",
    "            all_data = [] # An empty list to store each dataframe created.\n",
    "            for file_name in os.listdir(folder_path):\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "                # If statement that checks if the new file_path is a file, is yes it moves on and opens the file encoding it as latin-1 \n",
    "                # Latin-1 is used here, because the files could not be read with utf8. \n",
    "                # the read file is placed in a new variable caled text.\n",
    "                if os.path.isfile(file_path):\n",
    "                    with open(file_path, 'r', encoding=\"latin-1\") as file:\n",
    "                        text = file.read()\n",
    "                    \n",
    "\n",
    "                    # Regexing removing alle places where there are angle brackets.\n",
    "                    # sub - replaces the occurrance \n",
    "                    # . - means all characters\n",
    "                    # * - means zero or more occurances\n",
    "                    # ? - means zero or one coccurance. \n",
    "                    text = re.sub(r'<.*?>', '', text)\n",
    "                    doc = nlp(text) # use spacy nlp to create and find tokens defined by spacy.\n",
    "\n",
    "                    # Creating variables to be used below. \n",
    "                    noun_count =0  \n",
    "                    verb_count =0\n",
    "                    adjective_count = 0\n",
    "                    adverb_count = 0\n",
    "\n",
    "                    # For loop that counts the number of times each adj, noun, verb and adv accours, and adds one, by using spacys pos.\n",
    "                    for token in doc: \n",
    "                        if token.pos_ ==\"ADJ\":\n",
    "                            adjective_count +=1\n",
    "                        elif token.pos_ == \"NOUN\":\n",
    "                            noun_count += 1\n",
    "                        elif token.pos_ == \"VERB\":\n",
    "                            verb_count +=1\n",
    "                        elif token.pos_ == \"ADV\":\n",
    "                            adverb_count += 1\n",
    "\n",
    "                    # Finding the relative frequence by dividing a specific part of speech with the lenght of the text\n",
    "                    # and multiplying by 10 000. \n",
    "                    relative_freq_ADJ = (adjective_count/len(doc)) * 10000 \n",
    "                    relative_freq_ADJ = round(relative_freq_ADJ, 2)\n",
    "                    relative_freq_NOUN = (noun_count/len(doc)) * 10000\n",
    "                    relative_freq_NOUN = round(relative_freq_NOUN, 2)\n",
    "                    relative_freq_VERB = (verb_count/len(doc)) * 10000\n",
    "                    relative_freq_VERB = round(relative_freq_VERB, 2)\n",
    "                    relative_freq_ADV = (adverb_count/len(doc)) * 10000\n",
    "                    relative_freq_ADV = round(relative_freq_ADV, 2)\n",
    "\n",
    "\n",
    "                    # Finding Unique PER; LOC, ORG\n",
    "                    # creating empty list\n",
    "                    entities_PER = [] \n",
    "                    entities_LOC = []\n",
    "                    entities_ORG = []\n",
    "\n",
    "                    # get named entities and add to list \n",
    "                    # ent means entity\n",
    "                    # for loop that finds each word with either person, loc or org and appends to the matching variable \n",
    "                    for ent in doc.ents:  \n",
    "                        if ent.label_ == \"PERSON\": \n",
    "                            entities_PER.append(ent.text)\n",
    "                        elif ent.label_ == \"LOC\":\n",
    "                            entities_LOC.append(ent.text)\n",
    "                        elif ent.label_ == \"ORG\":\n",
    "                            entities_ORG.append(ent.text)\n",
    "\n",
    "                    # defining unique only with the set function\n",
    "                    unique_entities_PER = set(entities_PER) \n",
    "                    unique_entities_LOC = set(entities_LOC)\n",
    "                    unique_entities_ORG = set(entities_ORG)\n",
    "\n",
    "                    # Creating an empty list to store the touples, created below.\n",
    "                    touple_of_data = [] \n",
    "\n",
    "                    # Appending each variable together as a touples, and creating a dataframe out of the list. Specifying coloumns aswell.\n",
    "                    touple_of_data.append((file_name, relative_freq_NOUN, relative_freq_VERB, relative_freq_ADJ, relative_freq_ADV, unique_entities_PER, unique_entities_LOC, unique_entities_ORG))\n",
    "                    data = pd.DataFrame(touple_of_data, columns=['Filename', 'Noun Freq', 'Verb Freq', 'Adj Freq', 'Adv Freq', 'Unique PER', 'Unique LOC', 'Unique ORG'])\n",
    "                    \n",
    "                    # Appending each dataframe (which is only one row) into a new list created at the begining of this function. \n",
    "                    all_data.append(data)\n",
    "\n",
    "                # concating / appending all dataframes together to create one dataframe for each text.\n",
    "                final_data = pd.concat(all_data)\n",
    "                \n",
    "                # Saving the dataframe to folder out.\n",
    "                outpath = os.path.join(\"..\", \"out\", folder_name + \".csv\") \n",
    "                final_data.to_csv(outpath, index= False)\n",
    "\n",
    "    #return final_data\n",
    "                        \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying the path and running the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a variable which has the directory path.\n",
    "directory = os.path.join(\"..\", \"in\", \"USECorpus\")\n",
    "# Running the function\n",
    "dfs = find_attributes(directory)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ec8e9f70b97bcbb31cfe61427b5c1a80cbd771ccc80c0de317c984106d34155e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
