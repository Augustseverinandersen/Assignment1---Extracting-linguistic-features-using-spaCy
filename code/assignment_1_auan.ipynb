{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment concerns using ```spaCy``` to extract linguistic information from a corpus of texts.\n",
    "\n",
    "The corpus is an interesting one: *The Uppsala Student English Corpus (USE)*. All of the data is included in the folder called ```in``` but you can access more documentation via [this link](https://ota.bodleian.ox.ac.uk/repository/xmlui/handle/20.500.12024/2457).\n",
    "\n",
    "For this exercise, you should write some code which does the following:\n",
    "\n",
    "- Loop over each text file in the folder called ```in```\n",
    "- Extract the following information:\n",
    "    - Relative frequency of Nouns, Verbs, Adjective, and Adverbs per 10,000 words\n",
    "    - Total number of *unique* PER, LOC, ORGS\n",
    "- For each sub-folder (a1, a2, a3, ...) save a table which shows the following information:\n",
    "\n",
    "|Filename|RelFreq NOUN|RelFreq VERB|RelFreq ADJ|RelFreq ADV|Unique PER|Unique LOC|Unique ORG|\n",
    "|---|---|---|---|---|---|---|---|\n",
    "|file1.txt|---|---|---|---|---|---|---|\n",
    "|file2.txt|---|---|---|---|---|---|---|\n",
    "|etc|---|---|---|---|---|---|---|\n",
    "\n",
    "## Objective\n",
    "\n",
    "This assignment is designed to test that you can:\n",
    "\n",
    "1. Work with multiple input data arranged hierarchically in folders;\n",
    "2. Use ```spaCy``` to extract linguistic information from text data;\n",
    "3. Save those results in a clear way which can be shared or used for future analysis\n",
    "\n",
    "## Some notes\n",
    "\n",
    "- The data is arranged in various subfolders related to their content (see the [README](in/README.md) for more info). You'll need to think a little bit about how to do this. You should be able do it using a combination of things we've already looked at, such as ```os.listdir()```, ```os.path.join()```, and for loops.\n",
    "- The text files contain some extra information that such as document ID and other metadata that occurs between pointed brackets ```<>```. Make sure to remove these as part of your preprocessing steps!\n",
    "- There are 14 subfolders (a1, a2, a3, etc), so when completed the folder ```out``` should have 14 CSV files.\n",
    "\n",
    "## Additional comments\n",
    "\n",
    "Your code should include functions that you have written wherever possible. Try to break your code down into smaller self-contained parts, rather than having it as one long set of instructions.\n",
    "\n",
    "For this assignment, you are welcome to submit your code either as a Jupyter Notebook, or as ```.py``` script. If you do not know how to write ```.py``` scripts, don't worry - we're working towards that!\n",
    "\n",
    "Lastly, you are welcome to edit this README file to contain whatever informatio you like. Remember - documentation is important!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a spacy NLP class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\") # loads the entire model spacy into the variable nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on one text first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for extracting nouns and stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_attributes (filename):\n",
    "    filepath = os.path.join(\"..\", \"in\", \"USEcorpus\", \"a1\", filename) # define file path, with open file name\n",
    "    with open(filepath, \"r\", encoding=\"latin-1\") as file: # open the file and encode using utf 8\n",
    "        text = file.read()\n",
    "    text = re.sub(r'<.*?>', '', text) # remove all characters between < > \n",
    "    doc = nlp(text) # use spacy nlp  to create and find tokens.\n",
    "\n",
    "    # finding relFreg of nouns\n",
    "    noun_count =0 # creating empty variables \n",
    "    verb_count =0\n",
    "    adjective_count = 0\n",
    "    adverb_count = 0\n",
    "\n",
    "    for token in doc: # for loop that counts the number of times each adj, noun, verb and adv accours.\n",
    "        if token.pos_ ==\"ADJ\":\n",
    "            adjective_count +=1\n",
    "        elif token.pos_ == \"NOUN\":\n",
    "            noun_count += 1\n",
    "        elif token.pos_ == \"VERB\":\n",
    "            verb_count +=1\n",
    "        elif token.pos_ == \"ADV\":\n",
    "            adverb_count += 1\n",
    "\n",
    "    relative_freq_ADJ = (adjective_count/len(doc)) * 10000 # finding the relative frequence and storing in variable \n",
    "    relative_freq_ADJ = round(relative_freq_ADJ, 2)\n",
    "    relative_freq_NOUN = (noun_count/len(doc)) * 10000\n",
    "    relative_freq_NOUN = round(relative_freq_NOUN, 2)\n",
    "    relative_freq_VERB = (verb_count/len(doc)) * 10000\n",
    "    relative_freq_VERB = round(relative_freq_VERB, 2)\n",
    "    relative_freq_ADV = (adverb_count/len(doc)) * 10000\n",
    "    relative_freq_ADV = round(relative_freq_ADV, 2)\n",
    "    # Finding Unique PER; LOC, ORG\n",
    "    entities_PER = [] # creating empty list\n",
    "    entities_LOC = []\n",
    "    entities_ORG = []\n",
    "\n",
    "# get named entities and add to list \n",
    "    for ent in doc.ents: # ent means entity # for loop that finds each word with either person, loc or org and appends to the matching variable \n",
    "        if ent.label_ == \"PERSON\": \n",
    "            entities_PER.append(ent.text)\n",
    "        elif ent.label_ == \"LOC\":\n",
    "            entities_LOC.append(ent.text)\n",
    "        elif ent.label_ == \"ORG\":\n",
    "            entities_ORG.append(ent.text)\n",
    "\n",
    "    unique_entities_PER = set(entities_PER) # defining unique only \n",
    "    unique_entities_LOC = set(entities_LOC)\n",
    "    unique_entities_ORG = set(entities_ORG) # using set to find the unique entities in the list.\n",
    "    # checking to see if it has worked so far \n",
    "    #print(filename, relative_freq_NOUN, relative_freq_VERB, relative_freq_ADJ, relative_freq_ADV, unique_entities_PER, unique_entities_LOC, unique_entities_ORG)\n",
    "    \n",
    "    # creating a dictionary so i can store the data in a pandas dataframe \n",
    "    #datadic = [\n",
    "    #{\"Filename\": filename, \"RelFreq NOUN\": relative_freq_NOUN, \"RelFreq VERB\": relative_freq_VERB, \"RelFreq ADJ\": relative_freq_ADJ, \"RelFreq ADV\": relative_freq_ADV, \"Unique PER\": unique_entities_PER, \"Unique LOC\": unique_entities_LOC, \"Unique ORG\": unique_entities_ORG}\n",
    "#]\n",
    "    #touple_of_data = []\n",
    "    #for doc in [filename]:\n",
    "        #touple_of_data.append((doc, relative_freq_NOUN, relative_freq_VERB, relative_freq_ADJ, relative_freq_ADV, unique_entities_PER, unique_entities_LOC, unique_entities_ORG))\n",
    "    #print(touple_of_data)\n",
    "    all_tuples = []\n",
    "    touple_of_data = []\n",
    "    for doc in [filename]:\n",
    "        touple_of_data.append((filename, relative_freq_NOUN, relative_freq_VERB, relative_freq_ADJ, relative_freq_ADV, unique_entities_PER, unique_entities_LOC, unique_entities_ORG))\n",
    "        all_tuples.append(touple_of_data)\n",
    "    print(all_tuples)\n",
    "# creating a pandas dataframe, and storing in out folder as a csv file\n",
    "    #data = pd.DataFrame(touple_of_data)\n",
    "    #print(data)\n",
    "    #outpath = os.path.join(\"..\", \"out\", filename + \"annotations.csv\") # creating variable which works like a function for code below\n",
    "    #data.to_csv(outpath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('0100.a1.txt', 1533.05, 1223.63, 801.69, 534.46, set(), set(), set())]]\n"
     ]
    }
   ],
   "source": [
    "find_attributes(\"0100.a1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('0176.a1.txt', 1408.14, 1243.12, 649.06, 682.07, set(), set(), {'Visings√∂ Folk High School'})]]\n",
      "[[('3040.a1.txt', 1168.09, 1737.89, 940.17, 655.27, set(), set(), set())]]\n",
      "[[('2044.a1.txt', 1398.96, 1450.78, 556.99, 595.85, {'Marie Antoinette'}, set(), set())]]\n",
      "[[('1102.a1.txt', 1198.63, 1381.28, 730.59, 730.59, {'katt', 'superintendet Morse', 'Britsh', 'Minette Walters'}, {'Africa', 'Asia'}, {'instace'})]]\n",
      "[[('1029.a1.txt', 1342.04, 1330.17, 748.22, 439.43, set(), set(), set())]]\n",
      "[[('0218.a1.txt', 1404.36, 1452.78, 714.29, 447.94, set(), set(), set())]]\n",
      "[[('1043.a1.txt', 1356.32, 1264.37, 643.68, 678.16, set(), set(), set())]]\n",
      "[[('0200.a1.txt', 1261.17, 1320.75, 774.58, 705.06, {'Dickens', 'Austen', 'Shakespeare'}, {'Caribbean'}, {'Oxford'})]]\n",
      "[[('1074.a1.txt', 1552.68, 1293.9, 702.4, 674.68, set(), set(), set())]]\n",
      "[[('2047.a1.txt', 1520.91, 1318.12, 722.43, 671.74, set(), set(), set())]]\n",
      "[[('0187.a1.txt', 1298.7, 1428.57, 753.25, 753.25, set(), set(), {'the Cambridge Certificate in Advanced English'})]]\n",
      "[[('1026.a1.txt', 1000.0, 1344.83, 629.31, 456.9, set(), set(), set())]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mlistdir(filepath_new):\n\u001b[1;32m      3\u001b[0m     testone \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(filepath_new, file)\n\u001b[0;32m----> 4\u001b[0m     find_attributes(file)\n",
      "Cell \u001b[0;32mIn[47], line 6\u001b[0m, in \u001b[0;36mfind_attributes\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      4\u001b[0m     text \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39mread()\n\u001b[1;32m      5\u001b[0m text \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m<.*?>\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, text) \u001b[39m# remove all characters between < > \u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m doc \u001b[39m=\u001b[39m nlp(text) \u001b[39m# use spacy nlp  to create and find tokens.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39m# finding relFreg of nouns\u001b[39;00m\n\u001b[1;32m      9\u001b[0m noun_count \u001b[39m=\u001b[39m\u001b[39m0\u001b[39m \u001b[39m# creating empty variables \u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/spacy/language.py:1011\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     error_handler \u001b[39m=\u001b[39m proc\u001b[39m.\u001b[39mget_error_handler()\n\u001b[1;32m   1010\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1011\u001b[0m     doc \u001b[39m=\u001b[39m proc(doc, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcomponent_cfg\u001b[39m.\u001b[39;49mget(name, {}))  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1013\u001b[0m     \u001b[39m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[1;32m   1014\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE109\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "filepath_new = os.path.join(\"..\", \"in\", \"USEcorpus\", \"a1\")\n",
    "for file in os.listdir(filepath_new):\n",
    "    testone = os.path.join(filepath_new, file)\n",
    "    find_attributes(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'touple_of_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(touple_of_data)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'touple_of_data' is not defined"
     ]
    }
   ],
   "source": [
    "print(touple_of_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
