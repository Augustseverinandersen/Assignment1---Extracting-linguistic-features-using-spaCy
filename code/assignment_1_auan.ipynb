{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment concerns using ```spaCy``` to extract linguistic information from a corpus of texts.\n",
    "\n",
    "The corpus is an interesting one: *The Uppsala Student English Corpus (USE)*. All of the data is included in the folder called ```in``` but you can access more documentation via [this link](https://ota.bodleian.ox.ac.uk/repository/xmlui/handle/20.500.12024/2457).\n",
    "\n",
    "For this exercise, you should write some code which does the following:\n",
    "\n",
    "- Loop over each text file in the folder called ```in```\n",
    "- Extract the following information:\n",
    "    - Relative frequency of Nouns, Verbs, Adjective, and Adverbs per 10,000 words\n",
    "    - Total number of *unique* PER, LOC, ORGS\n",
    "- For each sub-folder (a1, a2, a3, ...) save a table which shows the following information:\n",
    "\n",
    "|Filename|RelFreq NOUN|RelFreq VERB|RelFreq ADJ|RelFreq ADV|Unique PER|Unique LOC|Unique ORG|\n",
    "|---|---|---|---|---|---|---|---|\n",
    "|file1.txt|---|---|---|---|---|---|---|\n",
    "|file2.txt|---|---|---|---|---|---|---|\n",
    "|etc|---|---|---|---|---|---|---|\n",
    "\n",
    "## Objective\n",
    "\n",
    "This assignment is designed to test that you can:\n",
    "\n",
    "1. Work with multiple input data arranged hierarchically in folders;\n",
    "2. Use ```spaCy``` to extract linguistic information from text data;\n",
    "3. Save those results in a clear way which can be shared or used for future analysis\n",
    "\n",
    "## Some notes\n",
    "\n",
    "- The data is arranged in various subfolders related to their content (see the [README](in/README.md) for more info). You'll need to think a little bit about how to do this. You should be able do it using a combination of things we've already looked at, such as ```os.listdir()```, ```os.path.join()```, and for loops.\n",
    "- The text files contain some extra information that such as document ID and other metadata that occurs between pointed brackets ```<>```. Make sure to remove these as part of your preprocessing steps!\n",
    "- There are 14 subfolders (a1, a2, a3, etc), so when completed the folder ```out``` should have 14 CSV files.\n",
    "\n",
    "## Additional comments\n",
    "\n",
    "Your code should include functions that you have written wherever possible. Try to break your code down into smaller self-contained parts, rather than having it as one long set of instructions.\n",
    "\n",
    "For this assignment, you are welcome to submit your code either as a Jupyter Notebook, or as ```.py``` script. If you do not know how to write ```.py``` scripts, don't worry - we're working towards that!\n",
    "\n",
    "Lastly, you are welcome to edit this README file to contain whatever informatio you like. Remember - documentation is important!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a spacy NLP class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\") # loads the entire model spacy into the variable nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on one text first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for extracting nouns and stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_attributes (filename):\n",
    "    filepath = os.path.join(\"..\", \"in\", \"USEcorpus\", \"c1\", filename) # define file path, with open file name\n",
    "    with open(filepath, \"r\", encoding=\"latin-1\") as file: # open the file and encode using utf 8\n",
    "        text = file.read()\n",
    "    text = re.sub(r'<.*?>', '', text) # remove all characters between < > \n",
    "    doc = nlp(text) # use spacy nlp  to create and find tokens.\n",
    "\n",
    "    # finding relFreg of nouns\n",
    "    noun_count =0 # creating empty variables \n",
    "    verb_count =0\n",
    "    adjective_count = 0\n",
    "    adverb_count = 0\n",
    "\n",
    "    for token in doc: # for loop that counts the number of times each adj, noun, verb and adv accours.\n",
    "        if token.pos_ ==\"ADJ\":\n",
    "            adjective_count +=1\n",
    "        elif token.pos_ == \"NOUN\":\n",
    "            noun_count += 1\n",
    "        elif token.pos_ == \"VERB\":\n",
    "            verb_count +=1\n",
    "        elif token.pos_ == \"ADV\":\n",
    "            adverb_count += 1\n",
    "\n",
    "    relative_freq_ADJ = (adjective_count/len(doc)) * 10000 # finding the relative frequence and storing in variable \n",
    "    relative_freq_ADJ = round(relative_freq_ADJ, 2)\n",
    "    relative_freq_NOUN = (noun_count/len(doc)) * 10000\n",
    "    relative_freq_NOUN = round(relative_freq_NOUN, 2)\n",
    "    relative_freq_VERB = (verb_count/len(doc)) * 10000\n",
    "    relative_freq_VERB = round(relative_freq_VERB, 2)\n",
    "    relative_freq_ADV = (adverb_count/len(doc)) * 10000\n",
    "    relative_freq_ADV = round(relative_freq_ADV, 2)\n",
    "    # Finding Unique PER; LOC, ORG\n",
    "    entities_PER = [] # creating empty list\n",
    "    entities_LOC = []\n",
    "    entities_ORG = []\n",
    "\n",
    "# get named entities and add to list \n",
    "    for ent in doc.ents: # ent means entity # for loop that finds each word with either person, loc or org and appends to the matching variable \n",
    "        if ent.label_ == \"PERSON\": \n",
    "            entities_PER.append(ent.text)\n",
    "        elif ent.label_ == \"LOC\":\n",
    "            entities_LOC.append(ent.text)\n",
    "        elif ent.label_ == \"ORG\":\n",
    "            entities_ORG.append(ent.text)\n",
    "\n",
    "    unique_entities_PER = set(entities_PER) # defining unique only \n",
    "    unique_entities_LOC = set(entities_LOC)\n",
    "    unique_entities_ORG = set(entities_ORG) # using set to find the unique entities in the list.\n",
    "   \n",
    "    touple_of_data = []\n",
    "    for doc in [filename]:\n",
    "        touple_of_data.append((filename, relative_freq_NOUN, relative_freq_VERB, relative_freq_ADJ, relative_freq_ADV, unique_entities_PER, unique_entities_LOC, unique_entities_ORG))\n",
    "        data = pd.DataFrame(touple_of_data, columns=['Filename', 'Noun Freq', 'Verb Freq', 'Adj Freq', 'Adv Freq', 'Unique PER', 'Unique LOC', 'Unique ORG'])\n",
    "    \n",
    "    return data\n",
    "# creating a pandas dataframe, and storing in out folder as a csv file\n",
    "    outpath = os.path.join(\"..\", \"out\", filename + \"annotations.csv\") # creating variable which works like a function for code below\n",
    "    #data.to_csv(outpath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Noun Freq</th>\n",
       "      <th>Verb Freq</th>\n",
       "      <th>Adj Freq</th>\n",
       "      <th>Adv Freq</th>\n",
       "      <th>Unique PER</th>\n",
       "      <th>Unique LOC</th>\n",
       "      <th>Unique ORG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0140.c1.txt</td>\n",
       "      <td>1573.58</td>\n",
       "      <td>933.55</td>\n",
       "      <td>472.89</td>\n",
       "      <td>403.59</td>\n",
       "      <td>{Geroge, Nick, Enoch Robinson, Benjy, George W...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{Hemingway's, Fury, Time, The Sound and the, C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0165.c1.txt</td>\n",
       "      <td>1742.49</td>\n",
       "      <td>816.41</td>\n",
       "      <td>580.83</td>\n",
       "      <td>284.32</td>\n",
       "      <td>{Nick, Benjy, Anderson, Quentin, Faulkner, Com...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{Sherwood Anderson's, Bentley, Fury}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0200.c1.txt</td>\n",
       "      <td>1177.65</td>\n",
       "      <td>1021.64</td>\n",
       "      <td>649.22</td>\n",
       "      <td>508.30</td>\n",
       "      <td>{Miriam, Catherine, Edgar Linton, Isabella, Ca...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{Penguin Classics, p, Nelly, Watts, Kettle, Ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0219.c1.txt</td>\n",
       "      <td>1379.31</td>\n",
       "      <td>974.80</td>\n",
       "      <td>563.66</td>\n",
       "      <td>484.08</td>\n",
       "      <td>{Catherine, Emily BrontÃ«'s, Terry, Emily, Long...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{ch.7, Heatcliff, T. Eagleton's, Nelly, St Ive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0238.c1.txt</td>\n",
       "      <td>1092.90</td>\n",
       "      <td>1163.15</td>\n",
       "      <td>398.13</td>\n",
       "      <td>288.84</td>\n",
       "      <td>{Emily BrontÃ«, Catherine, Edgar Linton, Cathy,...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{Longman, Kettle, Arnold, Prentize-Hall}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0501.c1.txt</td>\n",
       "      <td>1231.93</td>\n",
       "      <td>1025.46</td>\n",
       "      <td>461.11</td>\n",
       "      <td>426.70</td>\n",
       "      <td>{XXI, Catherine, Isabella, Nelly Dean, Hindley...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{P.47, Watts, Popular Classics, Hareton, Earns...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0502.c1.txt</td>\n",
       "      <td>1321.84</td>\n",
       "      <td>1219.67</td>\n",
       "      <td>434.23</td>\n",
       "      <td>408.68</td>\n",
       "      <td>{Catherine, Carl R., L. Cookson, Lockwood, ruf...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{Lockwood, Heatcliff, Nelly, Norgate, Grange}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Filename  Noun Freq  Verb Freq  Adj Freq  Adv Freq  \\\n",
       "0  0140.c1.txt    1573.58     933.55    472.89    403.59   \n",
       "0  0165.c1.txt    1742.49     816.41    580.83    284.32   \n",
       "0  0200.c1.txt    1177.65    1021.64    649.22    508.30   \n",
       "0  0219.c1.txt    1379.31     974.80    563.66    484.08   \n",
       "0  0238.c1.txt    1092.90    1163.15    398.13    288.84   \n",
       "0  0501.c1.txt    1231.93    1025.46    461.11    426.70   \n",
       "0  0502.c1.txt    1321.84    1219.67    434.23    408.68   \n",
       "\n",
       "                                          Unique PER Unique LOC  \\\n",
       "0  {Geroge, Nick, Enoch Robinson, Benjy, George W...         {}   \n",
       "0  {Nick, Benjy, Anderson, Quentin, Faulkner, Com...         {}   \n",
       "0  {Miriam, Catherine, Edgar Linton, Isabella, Ca...         {}   \n",
       "0  {Catherine, Emily BrontÃ«'s, Terry, Emily, Long...         {}   \n",
       "0  {Emily BrontÃ«, Catherine, Edgar Linton, Cathy,...         {}   \n",
       "0  {XXI, Catherine, Isabella, Nelly Dean, Hindley...         {}   \n",
       "0  {Catherine, Carl R., L. Cookson, Lockwood, ruf...         {}   \n",
       "\n",
       "                                          Unique ORG  \n",
       "0  {Hemingway's, Fury, Time, The Sound and the, C...  \n",
       "0               {Sherwood Anderson's, Bentley, Fury}  \n",
       "0  {Penguin Classics, p, Nelly, Watts, Kettle, Ar...  \n",
       "0  {ch.7, Heatcliff, T. Eagleton's, Nelly, St Ive...  \n",
       "0           {Longman, Kettle, Arnold, Prentize-Hall}  \n",
       "0  {P.47, Watts, Popular Classics, Hareton, Earns...  \n",
       "0      {Lockwood, Heatcliff, Nelly, Norgate, Grange}  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath_new = os.path.join(\"..\", \"in\", \"USEcorpus\", \"c1\")\n",
    "dataframes = []\n",
    "for file in os.listdir(filepath_new):\n",
    "    testone = os.path.join(filepath_new, file)\n",
    "    data = find_attributes(file)\n",
    "    dataframes.append(data)\n",
    "final_data = pd.concat(dataframes)\n",
    "final_data\n",
    "\n",
    "#outpath = os.path.join(\"..\", \"out\", \"df.csv\") # creating variable which works like a function for code below\n",
    "#final_data.to_csv(outpath)\n",
    "#print(dataframes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 80\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m     79\u001b[0m         \u001b[39m# find attributes for the file\u001b[39;00m\n\u001b[1;32m---> 80\u001b[0m         file_data \u001b[39m=\u001b[39m find_attributes(filename, subdir)\n\u001b[0;32m     81\u001b[0m         \u001b[39m# add\u001b[39;00m\n\u001b[0;32m     84\u001b[0m data_frames\n",
      "Cell \u001b[1;32mIn[24], line 7\u001b[0m, in \u001b[0;36mfind_attributes\u001b[1;34m(filename, subdir)\u001b[0m\n\u001b[0;32m      5\u001b[0m     text \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39mread()\n\u001b[0;32m      6\u001b[0m text \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m<.*?>\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, text)\n\u001b[1;32m----> 7\u001b[0m doc \u001b[39m=\u001b[39m nlp(text)\n\u001b[0;32m      9\u001b[0m \u001b[39m# finding relFreg of nouns\u001b[39;00m\n\u001b[0;32m     10\u001b[0m noun_count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\spacy\\language.py:1011\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m   1009\u001b[0m     error_handler \u001b[39m=\u001b[39m proc\u001b[39m.\u001b[39mget_error_handler()\n\u001b[0;32m   1010\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1011\u001b[0m     doc \u001b[39m=\u001b[39m proc(doc, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcomponent_cfg\u001b[39m.\u001b[39mget(name, {}))  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m   1012\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1013\u001b[0m     \u001b[39m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[0;32m   1014\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE109\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# function to find attributes for a single file\n",
    "def find_attributes(filename, subdir):\n",
    "    filepath = os.path.join(\"..\", \"in\", \"USEcorpus\", subdir, filename)\n",
    "    with open(filepath, \"r\", encoding=\"latin-1\") as file:\n",
    "        text = file.read()\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # finding relFreg of nouns\n",
    "    noun_count = 0\n",
    "    verb_count = 0\n",
    "    adjective_count = 0\n",
    "    adverb_count = 0\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"ADJ\":\n",
    "            adjective_count += 1\n",
    "        elif token.pos_ == \"NOUN\":\n",
    "            noun_count += 1\n",
    "        elif token.pos_ == \"VERB\":\n",
    "            verb_count += 1\n",
    "        elif token.pos_ == \"ADV\":\n",
    "            adverb_count += 1\n",
    "\n",
    "    relative_freq_ADJ = (adjective_count/len(doc)) * 10000\n",
    "    relative_freq_ADJ = round(relative_freq_ADJ, 2)\n",
    "    relative_freq_NOUN = (noun_count/len(doc)) * 10000\n",
    "    relative_freq_NOUN = round(relative_freq_NOUN, 2)\n",
    "    relative_freq_VERB = (verb_count/len(doc)) * 10000\n",
    "    relative_freq_VERB = round(relative_freq_VERB, 2)\n",
    "    relative_freq_ADV = (adverb_count/len(doc)) * 10000\n",
    "    relative_freq_ADV = round(relative_freq_ADV, 2)\n",
    "\n",
    "    # Finding Unique PER; LOC, ORG\n",
    "    entities_PER = []\n",
    "    entities_LOC = []\n",
    "    entities_ORG = []\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            entities_PER.append(ent.text)\n",
    "        elif ent.label_ == \"LOC\":\n",
    "            entities_LOC.append(ent.text)\n",
    "        elif ent.label_ == \"ORG\":\n",
    "            entities_ORG.append(ent.text)\n",
    "\n",
    "    unique_entities_PER = set(entities_PER)\n",
    "    unique_entities_LOC = set(entities_LOC)\n",
    "    unique_entities_ORG = set(entities_ORG)\n",
    "\n",
    "    data = pd.DataFrame({\n",
    "        'Filename': [filename],\n",
    "        'Noun Freq': [relative_freq_NOUN],\n",
    "        'Verb Freq': [relative_freq_VERB],\n",
    "        'Adj Freq': [relative_freq_ADJ],\n",
    "        'Adv Freq': [relative_freq_ADV],\n",
    "        'Unique PER': [unique_entities_PER],\n",
    "        'Unique LOC': [unique_entities_LOC],\n",
    "        'Unique ORG': [unique_entities_ORG]\n",
    "    })\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# loop over all subdirectories and files\n",
    "data_frames = []\n",
    "data_dir = os.path.join(\"..\", \"in\", \"USEcorpus\")\n",
    "for subdir in sorted(os.listdir(data_dir)):\n",
    "    # make the subdirectory path string\n",
    "    subdir_path = os.path.join(data_dir, subdir)\n",
    "    # skip any non-directory files\n",
    "    if not os.path.isdir(subdir_path):\n",
    "        continue\n",
    "    # for each file in the subdirectory\n",
    "    for filename in sorted(os.listdir(subdir_path)):\n",
    "        # skip any non-text files\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        # find attributes for the file\n",
    "        file_data = find_attributes(filename, subdir)\n",
    "        # add\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
