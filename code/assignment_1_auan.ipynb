{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment concerns using ```spaCy``` to extract linguistic information from a corpus of texts.\n",
    "\n",
    "The corpus is an interesting one: *The Uppsala Student English Corpus (USE)*. All of the data is included in the folder called ```in``` but you can access more documentation via [this link](https://ota.bodleian.ox.ac.uk/repository/xmlui/handle/20.500.12024/2457).\n",
    "\n",
    "For this exercise, you should write some code which does the following:\n",
    "\n",
    "- Loop over each text file in the folder called ```in```\n",
    "- Extract the following information:\n",
    "    - Relative frequency of Nouns, Verbs, Adjective, and Adverbs per 10,000 words\n",
    "    - Total number of *unique* PER, LOC, ORGS\n",
    "- For each sub-folder (a1, a2, a3, ...) save a table which shows the following information:\n",
    "\n",
    "|Filename|RelFreq NOUN|RelFreq VERB|RelFreq ADJ|RelFreq ADV|Unique PER|Unique LOC|Unique ORG|\n",
    "|---|---|---|---|---|---|---|---|\n",
    "|file1.txt|---|---|---|---|---|---|---|\n",
    "|file2.txt|---|---|---|---|---|---|---|\n",
    "|etc|---|---|---|---|---|---|---|\n",
    "\n",
    "## Objective\n",
    "\n",
    "This assignment is designed to test that you can:\n",
    "\n",
    "1. Work with multiple input data arranged hierarchically in folders;\n",
    "2. Use ```spaCy``` to extract linguistic information from text data;\n",
    "3. Save those results in a clear way which can be shared or used for future analysis\n",
    "\n",
    "## Some notes\n",
    "\n",
    "- The data is arranged in various subfolders related to their content (see the [README](in/README.md) for more info). You'll need to think a little bit about how to do this. You should be able do it using a combination of things we've already looked at, such as ```os.listdir()```, ```os.path.join()```, and for loops.\n",
    "- The text files contain some extra information that such as document ID and other metadata that occurs between pointed brackets ```<>```. Make sure to remove these as part of your preprocessing steps!\n",
    "- There are 14 subfolders (a1, a2, a3, etc), so when completed the folder ```out``` should have 14 CSV files.\n",
    "\n",
    "## Additional comments\n",
    "\n",
    "Your code should include functions that you have written wherever possible. Try to break your code down into smaller self-contained parts, rather than having it as one long set of instructions.\n",
    "\n",
    "For this assignment, you are welcome to submit your code either as a Jupyter Notebook, or as ```.py``` script. If you do not know how to write ```.py``` scripts, don't worry - we're working towards that!\n",
    "\n",
    "Lastly, you are welcome to edit this README file to contain whatever informatio you like. Remember - documentation is important!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a spacy NLP class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\") # loads the entire model spacy into the variable nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for finding Rel Freq and Unique "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_attributes(directory): # Making a function called find_attributes with the parameter folderpath\n",
    "    all_data = [] # An empty list to store each dataframe created.\n",
    "    \n",
    "    # Making a for loop that finds each file and the path to that file, and saves it in a variable folder_path\n",
    "    # os.listdir makes a list of the specified directory with all the files in the directory.\n",
    "    # os.path.join joins the \"file\" to the path for the file.\n",
    "    for folder_name in os.listdir(directory): \n",
    "        folder_path = os.path.join(directory, folder_name)\n",
    "\n",
    "        # Start by checking if the new variable is a directory, if true it moves on and finds the path to each file in the subfolder.\n",
    "        if os.path.isdir(folder_path):\n",
    "            for file_name in os.listdir(folder_path):\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "                # If statement that checks if the new file_path is a file, is yes it moves on and opens the file encoding it as latin-1 \n",
    "                # Latin-1 is used here, because the files could not be read with utf8. \n",
    "                # the read file is placed in a new variable caled text.\n",
    "                if os.path.isfile(file_path):\n",
    "                    with open(file_path, 'r', encoding=\"latin-1\") as file:\n",
    "                        text = file.read()\n",
    "\n",
    "                    # Regexing removing alle places where there are angle brackets.\n",
    "                    # sub - replaces the occurrance \n",
    "                    # . - means all characters\n",
    "                    # * - means zero or more occurances\n",
    "                    # ? - means zero or one coccurance. \n",
    "                    text = re.sub(r'<.*?>', '', text)\n",
    "                    doc = nlp(text) # use spacy nlp to create and find tokens defined by spacy.\n",
    "\n",
    "                    # Creating variables to be used below. \n",
    "                    noun_count =0  \n",
    "                    verb_count =0\n",
    "                    adjective_count = 0\n",
    "                    adverb_count = 0\n",
    "\n",
    "                    # For loop that counts the number of times each adj, noun, verb and adv accours, and adds one, by using spacys pos.\n",
    "                    for token in doc: \n",
    "                        if token.pos_ ==\"ADJ\":\n",
    "                            adjective_count +=1\n",
    "                        elif token.pos_ == \"NOUN\":\n",
    "                            noun_count += 1\n",
    "                        elif token.pos_ == \"VERB\":\n",
    "                            verb_count +=1\n",
    "                        elif token.pos_ == \"ADV\":\n",
    "                            adverb_count += 1\n",
    "\n",
    "                    # Finding the relative frequence by dividing a specific part of speech with the lenght of the text\n",
    "                    # and multiplying p√• 10 000. \n",
    "                    relative_freq_ADJ = (adjective_count/len(doc)) * 10000 \n",
    "                    relative_freq_ADJ = round(relative_freq_ADJ, 2)\n",
    "                    relative_freq_NOUN = (noun_count/len(doc)) * 10000\n",
    "                    relative_freq_NOUN = round(relative_freq_NOUN, 2)\n",
    "                    relative_freq_VERB = (verb_count/len(doc)) * 10000\n",
    "                    relative_freq_VERB = round(relative_freq_VERB, 2)\n",
    "                    relative_freq_ADV = (adverb_count/len(doc)) * 10000\n",
    "                    relative_freq_ADV = round(relative_freq_ADV, 2)\n",
    "\n",
    "\n",
    "                    # Finding Unique PER; LOC, ORG\n",
    "                    # creating empty list\n",
    "                    entities_PER = [] \n",
    "                    entities_LOC = []\n",
    "                    entities_ORG = []\n",
    "\n",
    "                    # get named entities and add to list \n",
    "                    # ent means entity\n",
    "                    # for loop that finds each word with either person, loc or org and appends to the matching variable \n",
    "                    for ent in doc.ents:  \n",
    "                        if ent.label_ == \"PERSON\": \n",
    "                            entities_PER.append(ent.text)\n",
    "                        elif ent.label_ == \"LOC\":\n",
    "                            entities_LOC.append(ent.text)\n",
    "                        elif ent.label_ == \"ORG\":\n",
    "                            entities_ORG.append(ent.text)\n",
    "\n",
    "                    # defining unique only with the set function\n",
    "                    unique_entities_PER = set(entities_PER) \n",
    "                    unique_entities_LOC = set(entities_LOC)\n",
    "                    unique_entities_ORG = set(entities_ORG)\n",
    "\n",
    "                    touple_of_data = []\n",
    "                    touple_of_data.append((file_name, relative_freq_NOUN, relative_freq_VERB, relative_freq_ADJ, relative_freq_ADV, unique_entities_PER, unique_entities_LOC, unique_entities_ORG))\n",
    "                    data = pd.DataFrame(touple_of_data, columns=['Filename', 'Noun Freq', 'Verb Freq', 'Adj Freq', 'Adv Freq', 'Unique PER', 'Unique LOC', 'Unique ORG'])\n",
    "                    all_data.append(data)\n",
    "\n",
    "                final_data = pd.concat(all_data)\n",
    "                \n",
    "                outpath = os.path.join(\"..\", \"out\", folder_name + \".csv\") # creating variable which works like a function for code below\n",
    "                final_data.to_csv(outpath, index= False)\n",
    "\n",
    "    return final_data\n",
    "                        \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying the path and running the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.path.join(\"..\", \"in\", \"USECorpus\")\n",
    "dfs = find_attributes(directory)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ec8e9f70b97bcbb31cfe61427b5c1a80cbd771ccc80c0de317c984106d34155e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
