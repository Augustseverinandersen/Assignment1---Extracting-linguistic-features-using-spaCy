{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment concerns using ```spaCy``` to extract linguistic information from a corpus of texts.\n",
    "\n",
    "The corpus is an interesting one: *The Uppsala Student English Corpus (USE)*. All of the data is included in the folder called ```in``` but you can access more documentation via [this link](https://ota.bodleian.ox.ac.uk/repository/xmlui/handle/20.500.12024/2457).\n",
    "\n",
    "For this exercise, you should write some code which does the following:\n",
    "\n",
    "- Loop over each text file in the folder called ```in```\n",
    "- Extract the following information:\n",
    "    - Relative frequency of Nouns, Verbs, Adjective, and Adverbs per 10,000 words\n",
    "    - Total number of *unique* PER, LOC, ORGS\n",
    "- For each sub-folder (a1, a2, a3, ...) save a table which shows the following information:\n",
    "\n",
    "|Filename|RelFreq NOUN|RelFreq VERB|RelFreq ADJ|RelFreq ADV|Unique PER|Unique LOC|Unique ORG|\n",
    "|---|---|---|---|---|---|---|---|\n",
    "|file1.txt|---|---|---|---|---|---|---|\n",
    "|file2.txt|---|---|---|---|---|---|---|\n",
    "|etc|---|---|---|---|---|---|---|\n",
    "\n",
    "## Objective\n",
    "\n",
    "This assignment is designed to test that you can:\n",
    "\n",
    "1. Work with multiple input data arranged hierarchically in folders;\n",
    "2. Use ```spaCy``` to extract linguistic information from text data;\n",
    "3. Save those results in a clear way which can be shared or used for future analysis\n",
    "\n",
    "## Some notes\n",
    "\n",
    "- The data is arranged in various subfolders related to their content (see the [README](in/README.md) for more info). You'll need to think a little bit about how to do this. You should be able do it using a combination of things we've already looked at, such as ```os.listdir()```, ```os.path.join()```, and for loops.\n",
    "- The text files contain some extra information that such as document ID and other metadata that occurs between pointed brackets ```<>```. Make sure to remove these as part of your preprocessing steps!\n",
    "- There are 14 subfolders (a1, a2, a3, etc), so when completed the folder ```out``` should have 14 CSV files.\n",
    "\n",
    "## Additional comments\n",
    "\n",
    "Your code should include functions that you have written wherever possible. Try to break your code down into smaller self-contained parts, rather than having it as one long set of instructions.\n",
    "\n",
    "For this assignment, you are welcome to submit your code either as a Jupyter Notebook, or as ```.py``` script. If you do not know how to write ```.py``` scripts, don't worry - we're working towards that!\n",
    "\n",
    "Lastly, you are welcome to edit this README file to contain whatever informatio you like. Remember - documentation is important!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a spacy NLP class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\") # loads the entire model spacy into the variable nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on one text first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for extracting nouns and stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_attributes (filename):\n",
    "    filepath = os.path.join(\"..\", \"in\", \"USEcorpus\", \"a1\", filename) # define file path, with open file name\n",
    "    with open(filepath, \"r\", encoding=\"latin-1\") as file: # open the file and encode using utf 8\n",
    "        text = file.read()\n",
    "    text = re.sub(r'<.*?>', '', text) # remove all characters between < > \n",
    "    doc = nlp(text) # use spacy nlp  to create and find tokens.\n",
    "\n",
    "    # finding relFreg of nouns\n",
    "    noun_count =0 # creating empty variables \n",
    "    verb_count =0\n",
    "    adjective_count = 0\n",
    "    adverb_count = 0\n",
    "\n",
    "    for token in doc: # for loop that counts the number of times each adj, noun, verb and adv accours.\n",
    "        if token.pos_ ==\"ADJ\":\n",
    "            adjective_count +=1\n",
    "        elif token.pos_ == \"NOUN\":\n",
    "            noun_count += 1\n",
    "        elif token.pos_ == \"VERB\":\n",
    "            verb_count +=1\n",
    "        elif token.pos_ == \"ADV\":\n",
    "            adverb_count += 1\n",
    "\n",
    "    relative_freq_ADJ = (adjective_count/len(doc)) * 10000 # finding the relative frequence and storing in variable \n",
    "    relative_freq_ADJ = round(relative_freq_ADJ, 2)\n",
    "    relative_freq_NOUN = (noun_count/len(doc)) * 10000\n",
    "    relative_freq_NOUN = round(relative_freq_NOUN, 2)\n",
    "    relative_freq_VERB = (verb_count/len(doc)) * 10000\n",
    "    relative_freq_VERB = round(relative_freq_VERB, 2)\n",
    "    relative_freq_ADV = (adverb_count/len(doc)) * 10000\n",
    "    relative_freq_ADV = round(relative_freq_ADV, 2)\n",
    "    # Finding Unique PER; LOC, ORG\n",
    "    entities_PER = [] # creating empty list\n",
    "    entities_LOC = []\n",
    "    entities_ORG = []\n",
    "\n",
    "# get named entities and add to list \n",
    "    for ent in doc.ents: # ent means entity # for loop that finds each word with either person, loc or org and appends to the matching variable \n",
    "        if ent.label_ == \"PERSON\": \n",
    "            entities_PER.append(ent.text)\n",
    "        elif ent.label_ == \"LOC\":\n",
    "            entities_LOC.append(ent.text)\n",
    "        elif ent.label_ == \"ORG\":\n",
    "            entities_ORG.append(ent.text)\n",
    "\n",
    "    unique_entities_PER = set(entities_PER) # defining unique only \n",
    "    unique_entities_LOC = set(entities_LOC)\n",
    "    unique_entities_ORG = set(entities_ORG) # using set to find the unique entities in the list.\n",
    "    # checking to see if it has worked so far \n",
    "    #print(filename, relative_freq_NOUN, relative_freq_VERB, relative_freq_ADJ, relative_freq_ADV, unique_entities_PER, unique_entities_LOC, unique_entities_ORG)\n",
    "    \n",
    "    # creating a dictionary so i can store the data in a pandas dataframe \n",
    "    #datadic = [\n",
    "    #{\"Filename\": filename, \"RelFreq NOUN\": relative_freq_NOUN, \"RelFreq VERB\": relative_freq_VERB, \"RelFreq ADJ\": relative_freq_ADJ, \"RelFreq ADV\": relative_freq_ADV, \"Unique PER\": unique_entities_PER, \"Unique LOC\": unique_entities_LOC, \"Unique ORG\": unique_entities_ORG}\n",
    "#]\n",
    "    #touple_of_data = []\n",
    "    #for doc in [filename]:\n",
    "        #touple_of_data.append((doc, relative_freq_NOUN, relative_freq_VERB, relative_freq_ADJ, relative_freq_ADV, unique_entities_PER, unique_entities_LOC, unique_entities_ORG))\n",
    "    #print(touple_of_data)\n",
    "    #all_tuples = []\n",
    "    touple_of_data = []\n",
    "    data = pd.DataFrame(touple_of_data, columns=['Filename', 'Noun Freq', 'Verb Freq', 'Adj Freq', 'Adv Freq', 'Unique PER', 'Unique LOC', 'Unique ORG'])\n",
    "    for doc in [filename]:\n",
    "        touple_of_data.append((filename, relative_freq_NOUN, relative_freq_VERB, relative_freq_ADJ, relative_freq_ADV, unique_entities_PER, unique_entities_LOC, unique_entities_ORG))\n",
    "    \n",
    "    print(data)\n",
    "# creating a pandas dataframe, and storing in out folder as a csv file\n",
    "    #outpath = os.path.join(\"..\", \"out\", filename + \"annotations.csv\") # creating variable which works like a function for code below\n",
    "    #data.to_csv(outpath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Filename  Noun Freq  Verb Freq  Adj Freq  Adv Freq Unique PER  \\\n",
      "0  0100.a1.txt    1533.05    1223.63    801.69    534.46         {}   \n",
      "\n",
      "  Unique LOC Unique ORG  \n",
      "0         {}         {}  \n"
     ]
    }
   ],
   "source": [
    "find_attributes(\"0100.a1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Filename  Noun Freq  Verb Freq  Adj Freq  Adv Freq Unique PER  \\\n",
      "0  0176.a1.txt    1408.14    1243.12    649.06    682.07         {}   \n",
      "\n",
      "  Unique LOC                   Unique ORG  \n",
      "0         {}  {Visingsö Folk High School}  \n",
      "      Filename  Noun Freq  Verb Freq  Adj Freq  Adv Freq Unique PER  \\\n",
      "0  3040.a1.txt    1168.09    1737.89    940.17    655.27         {}   \n",
      "\n",
      "  Unique LOC Unique ORG  \n",
      "0         {}         {}  \n",
      "      Filename  Noun Freq  Verb Freq  Adj Freq  Adv Freq          Unique PER  \\\n",
      "0  2044.a1.txt    1398.96    1450.78    556.99    595.85  {Marie Antoinette}   \n",
      "\n",
      "  Unique LOC Unique ORG  \n",
      "0         {}         {}  \n",
      "      Filename  Noun Freq  Verb Freq  Adj Freq  Adv Freq  \\\n",
      "0  1102.a1.txt    1198.63    1381.28    730.59    730.59   \n",
      "\n",
      "                                          Unique PER      Unique LOC  \\\n",
      "0  {superintendet Morse, katt, Britsh, Minette Wa...  {Asia, Africa}   \n",
      "\n",
      "  Unique ORG  \n",
      "0  {instace}  \n",
      "      Filename  Noun Freq  Verb Freq  Adj Freq  Adv Freq Unique PER  \\\n",
      "0  1029.a1.txt    1342.04    1330.17    748.22    439.43         {}   \n",
      "\n",
      "  Unique LOC Unique ORG  \n",
      "0         {}         {}  \n",
      "      Filename  Noun Freq  Verb Freq  Adj Freq  Adv Freq Unique PER  \\\n",
      "0  0218.a1.txt    1404.36    1452.78    714.29    447.94         {}   \n",
      "\n",
      "  Unique LOC Unique ORG  \n",
      "0         {}         {}  \n",
      "      Filename  Noun Freq  Verb Freq  Adj Freq  Adv Freq Unique PER  \\\n",
      "0  1043.a1.txt    1356.32    1264.37    643.68    678.16         {}   \n",
      "\n",
      "  Unique LOC Unique ORG  \n",
      "0         {}         {}  \n",
      "      Filename  Noun Freq  Verb Freq  Adj Freq  Adv Freq  \\\n",
      "0  0200.a1.txt    1261.17    1320.75    774.58    705.06   \n",
      "\n",
      "                       Unique PER   Unique LOC Unique ORG  \n",
      "0  {Dickens, Shakespeare, Austen}  {Caribbean}   {Oxford}  \n",
      "      Filename  Noun Freq  Verb Freq  Adj Freq  Adv Freq Unique PER  \\\n",
      "0  1074.a1.txt    1552.68     1293.9     702.4    674.68         {}   \n",
      "\n",
      "  Unique LOC Unique ORG  \n",
      "0         {}         {}  \n",
      "      Filename  Noun Freq  Verb Freq  Adj Freq  Adv Freq Unique PER  \\\n",
      "0  2047.a1.txt    1520.91    1318.12    722.43    671.74         {}   \n",
      "\n",
      "  Unique LOC Unique ORG  \n",
      "0         {}         {}  \n",
      "      Filename  Noun Freq  Verb Freq  Adj Freq  Adv Freq Unique PER  \\\n",
      "0  0187.a1.txt     1298.7    1428.57    753.25    753.25         {}   \n",
      "\n",
      "  Unique LOC                                       Unique ORG  \n",
      "0         {}  {the Cambridge Certificate in Advanced English}  \n",
      "      Filename  Noun Freq  Verb Freq  Adj Freq  Adv Freq Unique PER  \\\n",
      "0  1026.a1.txt     1000.0    1344.83    629.31     456.9         {}   \n",
      "\n",
      "  Unique LOC Unique ORG  \n",
      "0         {}         {}  \n",
      "      Filename  Noun Freq  Verb Freq  Adj Freq  Adv Freq        Unique PER  \\\n",
      "0  2012.a1.txt    1522.31    1102.36     839.9    708.66  {Michael Abrash}   \n",
      "\n",
      "  Unique LOC                                        Unique ORG  \n",
      "0         {}  {Uppsala University, Simpsons, Computer Science}  \n",
      "      Filename  Noun Freq  Verb Freq  Adj Freq  Adv Freq Unique PER  \\\n",
      "0  2067.a1.txt    1192.66    1376.15    550.46    576.67         {}   \n",
      "\n",
      "  Unique LOC Unique ORG  \n",
      "0         {}         {}  \n",
      "      Filename  Noun Freq  Verb Freq  Adj Freq  Adv Freq Unique PER  \\\n",
      "0  2013.a1.txt    1290.98    1127.05    676.23    891.39  {Cockney}   \n",
      "\n",
      "  Unique LOC Unique ORG  \n",
      "0         {}         {}  \n",
      "      Filename  Noun Freq  Verb Freq  Adj Freq  Adv Freq     Unique PER  \\\n",
      "0  0194.a1.txt    1109.97    1469.68    791.37    688.59  {David Lodge}   \n",
      "\n",
      "  Unique LOC                      Unique ORG  \n",
      "0         {}  {the institute of education I}  \n",
      "      Filename  Noun Freq  Verb Freq  Adj Freq  Adv Freq    Unique PER  \\\n",
      "0  0123.a1.txt    1245.24    1359.59     597.2     775.1  {Tony Blair}   \n",
      "\n",
      "  Unique LOC Unique ORG  \n",
      "0         {}         {}  \n",
      "      Filename  Noun Freq  Verb Freq  Adj Freq  Adv Freq Unique PER  \\\n",
      "0  1064.a1.txt    1311.08    1299.17    762.81    798.57         {}   \n",
      "\n",
      "  Unique LOC Unique ORG  \n",
      "0         {}         {}  \n",
      "      Filename  Noun Freq  Verb Freq  Adj Freq  Adv Freq Unique PER  \\\n",
      "0  2037.a1.txt    1599.44     1182.2   1070.93    598.05         {}   \n",
      "\n",
      "  Unique LOC Unique ORG  \n",
      "0         {}         {}  \n",
      "      Filename  Noun Freq  Verb Freq  Adj Freq  Adv Freq     Unique PER  \\\n",
      "0  0204.a1.txt    1466.99    1112.47    611.25    513.45  {Björn Skifs}   \n",
      "\n",
      "  Unique LOC Unique ORG  \n",
      "0         {}      {CNN}  \n",
      "      Filename  Noun Freq  Verb Freq  Adj Freq  Adv Freq Unique PER  \\\n",
      "0  0214.a1.txt    1169.45    1264.92    584.73    823.39     {Bean}   \n",
      "\n",
      "  Unique LOC Unique ORG  \n",
      "0         {}         {}  \n",
      "      Filename  Noun Freq  Verb Freq  Adj Freq  Adv Freq         Unique PER  \\\n",
      "0  2074.a1.txt    1412.21     979.64    865.14    903.31  {Allthough, summ}   \n",
      "\n",
      "  Unique LOC                                         Unique ORG  \n",
      "0         {}  {Newsweek, the University of Uppsala, Focus an...  \n",
      "      Filename  Noun Freq  Verb Freq  Adj Freq  Adv Freq Unique PER  \\\n",
      "0  1054.a1.txt    1566.92    1479.87    544.07    489.66         {}   \n",
      "\n",
      "  Unique LOC Unique ORG  \n",
      "0         {}    {Times}  \n",
      "      Filename  Noun Freq  Verb Freq  Adj Freq  Adv Freq Unique PER  \\\n",
      "0  2039.a1.txt    1343.64    1486.33    689.66    665.87         {}   \n",
      "\n",
      "  Unique LOC Unique ORG  \n",
      "0         {}         {}  \n",
      "      Filename  Noun Freq  Verb Freq  Adj Freq  Adv Freq Unique PER  \\\n",
      "0  2066.a1.txt    1412.07    1347.88   1001.28    731.71         {}   \n",
      "\n",
      "  Unique LOC Unique ORG  \n",
      "0         {}         {}  \n",
      "      Filename  Noun Freq  Verb Freq  Adj Freq  Adv Freq Unique PER  \\\n",
      "0  0125.a1.txt    1828.84    1113.72    844.08    586.17         {}   \n",
      "\n",
      "  Unique LOC Unique ORG  \n",
      "0     {Asia}         {}  \n",
      "      Filename  Noun Freq  Verb Freq  Adj Freq  Adv Freq Unique PER  \\\n",
      "0  2000.a1.txt    1473.88     970.15    727.61    522.39         {}   \n",
      "\n",
      "  Unique LOC Unique ORG  \n",
      "0         {}         {}  \n",
      "      Filename  Noun Freq  Verb Freq  Adj Freq  Adv Freq Unique PER  \\\n",
      "0  0195.a1.txt    1254.61    1500.62    639.61    578.11         {}   \n",
      "\n",
      "  Unique LOC Unique ORG  \n",
      "0         {}         {}  \n",
      "      Filename  Noun Freq  Verb Freq  Adj Freq  Adv Freq Unique PER  \\\n",
      "0  1031.a1.txt     1169.0    1410.42     597.2     584.5         {}   \n",
      "\n",
      "  Unique LOC Unique ORG  \n",
      "0         {}   {Groups}  \n",
      "      Filename  Noun Freq  Verb Freq  Adj Freq  Adv Freq Unique PER  \\\n",
      "0  0134.a1.txt    1355.93    1246.97    726.39   1162.23         {}   \n",
      "\n",
      "  Unique LOC Unique ORG  \n",
      "0         {}         {}  \n",
      "      Filename  Noun Freq  Verb Freq  Adj Freq  Adv Freq Unique PER  \\\n",
      "0  2018.a1.txt    1349.83    1664.79    528.68    449.94         {}   \n",
      "\n",
      "  Unique LOC Unique ORG  \n",
      "0         {}         {}  \n",
      "      Filename  Noun Freq  Verb Freq  Adj Freq  Adv Freq Unique PER  \\\n",
      "0  3041.a1.txt    1077.84    1287.43    718.56    389.22         {}   \n",
      "\n",
      "  Unique LOC Unique ORG  \n",
      "0         {}         {}  \n",
      "      Filename  Noun Freq  Verb Freq  Adj Freq  Adv Freq  \\\n",
      "0  1060.a1.txt     823.27     1383.1    515.92    680.57   \n",
      "\n",
      "                                          Unique PER Unique LOC Unique ORG  \n",
      "0  {wright, recognice, Sarah, Lewis, Hastings, mr...         {}         {}  \n",
      "      Filename  Noun Freq  Verb Freq  Adj Freq  Adv Freq Unique PER  \\\n",
      "0  1066.a1.txt    1351.04    1224.02    588.91    600.46   {Glover}   \n",
      "\n",
      "  Unique LOC Unique ORG  \n",
      "0         {}         {}  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mlistdir(filepath_new):\n\u001b[1;32m      3\u001b[0m     testone \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(filepath_new, file)\n\u001b[0;32m----> 4\u001b[0m     find_attributes(file)\n",
      "Cell \u001b[0;32mIn[44], line 6\u001b[0m, in \u001b[0;36mfind_attributes\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      4\u001b[0m     text \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39mread()\n\u001b[1;32m      5\u001b[0m text \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m<.*?>\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, text) \u001b[39m# remove all characters between < > \u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m doc \u001b[39m=\u001b[39m nlp(text) \u001b[39m# use spacy nlp  to create and find tokens.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39m# finding relFreg of nouns\u001b[39;00m\n\u001b[1;32m      9\u001b[0m noun_count \u001b[39m=\u001b[39m\u001b[39m0\u001b[39m \u001b[39m# creating empty variables \u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/spacy/language.py:1011\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     error_handler \u001b[39m=\u001b[39m proc\u001b[39m.\u001b[39mget_error_handler()\n\u001b[1;32m   1010\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1011\u001b[0m     doc \u001b[39m=\u001b[39m proc(doc, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcomponent_cfg\u001b[39m.\u001b[39;49mget(name, {}))  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1013\u001b[0m     \u001b[39m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[1;32m   1014\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE109\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/spacy/pipeline/trainable_pipe.pyx:52\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/spacy/pipeline/tok2vec.py:125\u001b[0m, in \u001b[0;36mTok2Vec.predict\u001b[0;34m(self, docs)\u001b[0m\n\u001b[1;32m    123\u001b[0m     width \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mget_dim(\u001b[39m\"\u001b[39m\u001b[39mnO\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    124\u001b[0m     \u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39malloc((\u001b[39m0\u001b[39m, width)) \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m docs]\n\u001b[0;32m--> 125\u001b[0m tokvecs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict(docs)\n\u001b[1;32m    126\u001b[0m \u001b[39mreturn\u001b[39;00m tokvecs\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/thinc/model.py:315\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X: InT) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m OutT:\n\u001b[1;32m    312\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[39m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/thinc/layers/chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[1;32m     56\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     57\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/thinc/layers/chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[1;32m     56\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     57\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/thinc/layers/concatenate.py:44\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(model: Model[InT, OutT], X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m---> 44\u001b[0m     Ys, callbacks \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m[layer(X, is_train\u001b[39m=\u001b[39mis_train) \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers])\n\u001b[1;32m     45\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(Ys[\u001b[39m0\u001b[39m], \u001b[39mlist\u001b[39m):\n\u001b[1;32m     46\u001b[0m         data_l, backprop \u001b[39m=\u001b[39m _list_forward(model, X, Ys, callbacks, is_train)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/thinc/layers/concatenate.py:44\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(model: Model[InT, OutT], X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m---> 44\u001b[0m     Ys, callbacks \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m[layer(X, is_train\u001b[39m=\u001b[39;49mis_train) \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers])\n\u001b[1;32m     45\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(Ys[\u001b[39m0\u001b[39m], \u001b[39mlist\u001b[39m):\n\u001b[1;32m     46\u001b[0m         data_l, backprop \u001b[39m=\u001b[39m _list_forward(model, X, Ys, callbacks, is_train)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/thinc/layers/chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[1;32m     56\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     57\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/spacy/ml/featureextractor.py:20\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, docs, is_train)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m docs:\n\u001b[1;32m     19\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(doc, \u001b[39m\"\u001b[39m\u001b[39mto_array\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m---> 20\u001b[0m         attrs \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39;49mto_array(columns)\n\u001b[1;32m     21\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m         attrs \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39mdoc\u001b[39m.\u001b[39mto_array(columns)[doc\u001b[39m.\u001b[39mstart : doc\u001b[39m.\u001b[39mend]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "filepath_new = os.path.join(\"..\", \"in\", \"USEcorpus\", \"a1\")\n",
    "for file in os.listdir(filepath_new):\n",
    "    testone = os.path.join(filepath_new, file)\n",
    "    find_attributes(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'touple_of_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(touple_of_data)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'touple_of_data' is not defined"
     ]
    }
   ],
   "source": [
    "print(touple_of_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
